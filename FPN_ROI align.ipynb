{"cells":[{"cell_type":"code","source":["!pip install wget\n","!pip install ftfy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cj5Qw1hBpMyh","executionInfo":{"status":"ok","timestamp":1650137084961,"user_tz":240,"elapsed":9365,"user":{"displayName":"Wu Kingsley","userId":"01468963787757959193"}},"outputId":"b047420c-a6c5-4c6b-9f4f-628b9ce45586"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.1.1)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n"]}]},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2\n","\n","from google.colab import drive\n","\n","drive.mount(\"/content/drive\")\n","import os\n","import sys\n","\n","# TODO: Fill in the Google Drive path where you uploaded the assignment\n","# Example: If you create a WI2022 folder and put all the files under A5 folder, then \"WI2022/A5\"\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = './eecs545pro'\n","GOOGLE_DRIVE_PATH = os.path.join(\"drive\", \"My Drive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))\n","\n","\n","# Add to sys so we can import .py files.\n","sys.path.append(GOOGLE_DRIVE_PATH)"],"metadata":{"id":"o8LSFtlsUmcD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650137085518,"user_tz":240,"elapsed":567,"user":{"displayName":"Wu Kingsley","userId":"01468963787757959193"}},"outputId":"8066f591-ceba-4ed5-9428-8a437766a87c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","['a4_helper.py', 'simple_tokenizer.py', 'clip.py', 'bpe_simple_vocab_16e6.txt.gz', '__init__.py', 'model.py', 'common.py', 'FPN_ROI.py', 'clip', 'A4', 'mAP', 'eecs598', '__pycache__', 'FPN_ROI align.ipynb', 'voc07_val.json', 'voc07_train.json']\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"LqTIzJttTvN4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650137093269,"user_tz":240,"elapsed":7754,"user":{"displayName":"Wu Kingsley","userId":"01468963787757959193"}},"outputId":"740cbd26-64cf-4f21-a1f3-428a9d7c61b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["<a4_helper.VOC2007DetectionTiny object at 0x7f39df4a4990>\n"]}],"source":["from a4_helper import *\n","from eecs598 import reset_seed\n","import multiprocessing\n","\n","\n","# Set a few constants related to data loading.\n","NUM_CLASSES = 20\n","BATCH_SIZE = 16\n","IMAGE_SHAPE = (224*4, 224*4)\n","NUM_WORKERS = multiprocessing.cpu_count()\n","from a4_helper import VOC2007DetectionTiny\n","\n","train_dataset = VOC2007DetectionTiny(\n","    GOOGLE_DRIVE_PATH_AFTER_MYDRIVE, \"train\", image_size=IMAGE_SHAPE[0],\n","    download=False  # True (for the first time)\n",")\n","val_dataset = VOC2007DetectionTiny(GOOGLE_DRIVE_PATH_AFTER_MYDRIVE, \"val\", image_size=IMAGE_SHAPE[0])\n","DEVICE = 'cpu'\n","\n","print(train_dataset)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"QXkRbrj9TvN7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650137096089,"user_tz":240,"elapsed":2832,"user":{"displayName":"Wu Kingsley","userId":"01468963787757959193"}},"outputId":"8fb56ac3-4259-4c15-c98b-848535a17bed"},"outputs":[{"output_type":"stream","name":"stdout","text":["For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([2, 64, 28, 28])\n","Shape of p4 features: torch.Size([2, 64, 14, 14])\n","Shape of p5 features: torch.Size([2, 64, 7, 7])\n","Shape of p6 features: torch.Size([2, 64, 4, 4])\n","Shape of p7 features: torch.Size([2, 64, 2, 2])\n"]}],"source":["import torch\n","from common import DetectorBackboneWithFPN\n","import torchvision\n","\n","\n","# sanity check\n","backbone = DetectorBackboneWithFPN(out_channels=64)\n","\n","dummy_images = torch.randn(2, 3, 224, 224)\n","\n","dummy_fpn_feats = backbone(dummy_images)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"CTH3bfg-TvN7","executionInfo":{"status":"ok","timestamp":1650137096090,"user_tz":240,"elapsed":7,"user":{"displayName":"Wu Kingsley","userId":"01468963787757959193"}}},"outputs":[],"source":["# # model block sanity check\n","# model = torchvision.models.regnet_x_400mf(pretrained=True)\n","# train_nodes, eval_nodes = torchvision.models.feature_extraction.get_graph_node_names(model)\n","\n","# print(train_nodes)\n","# print(eval_nodes)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"U3P_CygbTvN8","executionInfo":{"status":"ok","timestamp":1650137096091,"user_tz":240,"elapsed":7,"user":{"displayName":"Wu Kingsley","userId":"01468963787757959193"}}},"outputs":[],"source":["# FPN trainability check\n","\n","# from torch import nn\n","# from a4_helper import train_detector\n","# from common import DetectorBackboneWithFPN\n","# from FPN_ROI import RPN\n","# reset_seed(0)\n","\n","# # Take equally spaced examples from training dataset to make a subset.\n","# small_dataset = torch.utils.data.Subset(\n","#     train_dataset,\n","#     torch.linspace(0, len(train_dataset) - 1, steps=BATCH_SIZE * 10).long()\n","# )\n","# small_train_loader = torch.utils.data.DataLoader(\n","#     small_dataset, batch_size=BATCH_SIZE, pin_memory=True\n","# )\n","\n","# # Create a wrapper module to contain backbone + RPN:\n","# class FirstStage(nn.Module):\n","#     def __init__(self, fpn_channels: int):\n","#         super().__init__()\n","#         self.backbone = DetectorBackboneWithFPN(out_channels=fpn_channels)\n","#         self.rpn = RPN(\n","#             fpn_channels=fpn_channels,\n","#             # Simple stem of two layers:\n","#             stem_channels=[fpn_channels, fpn_channels],\n","#             batch_size_per_image=16,\n","#             anchor_stride_scale=8,\n","#             anchor_aspect_ratios=[0.5, 1.0, 2.0],\n","#             anchor_iou_thresholds=(0.3, 0.6),\n","#         )\n","\n","#     def forward(self, images, gt_boxes=None):\n","#         feats_per_fpn_level = self.backbone(images)\n","#         return self.rpn(feats_per_fpn_level, self.backbone.fpn_strides, gt_boxes)\n","\n","\n","# first_stage = FirstStage(fpn_channels=64).to(DEVICE)\n","\n","# train_detector(\n","#     first_stage,\n","#     small_train_loader,\n","#     learning_rate=8e-3,\n","#     max_iters=1000,\n","#     log_period=20,\n","#     device=DEVICE,\n","# )"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"mw1eHnWmTvN9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650137096882,"user_tz":240,"elapsed":797,"user":{"displayName":"Wu Kingsley","userId":"01468963787757959193"}},"outputId":"6c3d0790-06db-4612-ba98-1070d74cec1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["image batch has shape : torch.Size([16, 3, 896, 896])\n","gt_boxes has shape    : torch.Size([16, 40, 5])\n","Five boxes per image  :\n","tensor([[[195.7477, 260.9970, 720.4324, 726.4865,   6.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n","\n","        [[313.3846, 152.6154, 544.7692, 489.8461,  14.0000],\n","         [ 79.5385, 192.0000, 850.0000, 827.0769,  12.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n","\n","        [[ 24.1437, 395.0060, 657.2455, 896.0000,   1.0000],\n","         [617.0060, 368.1796, 896.0000, 896.0000,   1.0000],\n","         [  5.3653,   0.0000, 313.8683, 767.8922,  14.0000],\n","         [  8.0479,   0.0000, 651.8802, 896.0000,  14.0000],\n","         [603.5928,   0.0000, 896.0000, 896.0000,  14.0000]],\n","\n","        [[ 18.1622, 336.3364, 682.7628, 570.4264,   6.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n","\n","        [[  0.0000, 248.7118, 846.7295, 583.5160,   0.0000],\n","         [ 75.0854, 280.5979, 279.1566, 392.1993,   0.0000],\n","         [272.7794, 573.9502, 330.1744, 730.1921,  14.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n","\n","        [[  0.0000, 261.9453, 896.0000, 643.8470,   0.0000],\n","         [866.6448, 489.6175, 896.0000, 553.2678,   0.0000],\n","         [631.6284, 460.2404, 842.1639, 545.9235,   0.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n","\n","        [[288.7111, 241.6444, 896.0000, 821.5556,  18.0000],\n","         [350.9333, 206.8000, 828.8000, 395.9556,  18.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n","\n","        [[  0.0000, 229.3760, 307.3626, 862.5494,  14.0000],\n","         [371.8746, 234.1547, 896.0000, 759.8080,  14.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n","\n","        [[ 72.8675,   0.0000, 860.9156, 702.3856,  11.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n","\n","        [[483.4269,  85.5881, 896.0000, 789.0149,  18.0000],\n","         [  0.0000,  96.2866, 408.5374, 799.7134,  18.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n","\n","        [[  0.0000,   2.6907, 771.5555, 887.9279,   8.0000],\n","         [ 42.3784, 271.7598, 615.4955, 573.1171,   7.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n","\n","        [[  0.0000,   2.6907, 896.0000, 839.4955,   6.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n","\n","        [[138.5813, 106.6587, 695.2960, 896.0000,   2.0000],\n","         [  4.7787,   0.0000, 721.5787, 896.0000,  14.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n","\n","        [[513.2492,  29.5976, 896.0000, 750.7027,   3.0000],\n","         [271.0871, 575.8078, 532.0840, 678.0541,   3.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n","\n","        [[  1.8065,  38.5484, 814.7097, 896.0000,   2.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]],\n","\n","        [[350.3707, 446.8053, 395.7680, 549.5467,  14.0000],\n","         [429.2186, 434.8586, 505.6773, 618.8373,  14.0000],\n","         [493.7307, 449.1947, 555.8533, 618.8373,  14.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000],\n","         [ -1.0000,  -1.0000,  -1.0000,  -1.0000,  -1.0000]]])\n","tensor([[-0.8678, -0.8849, -0.9020,  ..., -0.9705, -0.9705, -0.9877],\n","        [-0.8678, -0.8849, -0.9020,  ..., -0.9705, -0.9705, -0.9877],\n","        [-0.8678, -0.8849, -0.9020,  ..., -0.9534, -0.9534, -0.9705],\n","        ...,\n","        [-0.6794, -0.6623, -0.6794,  ..., -0.9705, -1.0048, -1.0390],\n","        [-0.7137, -0.6965, -0.6965,  ..., -0.9877, -1.0048, -1.0390],\n","        [-0.7137, -0.6965, -0.6965,  ..., -0.9877, -1.0048, -1.0390]])\n"]}],"source":["# load data\n","\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=BATCH_SIZE, pin_memory=True\n",")\n","\n","val_loader = torch.utils.data.DataLoader(\n","    val_dataset, batch_size=1, pin_memory=True\n",")\n","\n","train_loader_iter = iter(train_loader)\n","image_paths, images, gt_boxes = train_loader_iter.next()\n","\n","# print(f\"image paths           : {image_paths}\")\n","print(f\"image batch has shape : {images.shape}\")\n","print(f\"gt_boxes has shape    : {gt_boxes.shape}\")\n","\n","print(f\"Five boxes per image  :\")\n","print(gt_boxes[:, :5, :])\n","print(images[0,0,:,:])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4DZ9njrmTvN9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"446dd85f-2d0a-4c24-86af-f3d7151f8ee4"},"outputs":[{"output_type":"stream","name":"stdout","text":["For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([33, 3, 224, 224])\n","torch.Size([33, 512])\n","[Iter 0][loss: 3.970][loss_rpn_obj: 0.694][loss_rpn_box: 0.238][loss_cls: 3.038]\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([41, 3, 224, 224])\n","torch.Size([41, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([29, 3, 224, 224])\n","torch.Size([29, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([41, 3, 224, 224])\n","torch.Size([41, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([50, 3, 224, 224])\n","torch.Size([50, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([52, 3, 224, 224])\n","torch.Size([52, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([69, 3, 224, 224])\n","torch.Size([69, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([73, 3, 224, 224])\n","torch.Size([73, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([61, 3, 224, 224])\n","torch.Size([61, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([50, 3, 224, 224])\n","torch.Size([50, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([54, 3, 224, 224])\n","torch.Size([54, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([37, 3, 224, 224])\n","torch.Size([37, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([34, 3, 224, 224])\n","torch.Size([34, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([46, 3, 224, 224])\n","torch.Size([46, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([40, 3, 224, 224])\n","torch.Size([40, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([42, 3, 224, 224])\n","torch.Size([42, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([72, 3, 224, 224])\n","torch.Size([72, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([54, 3, 224, 224])\n","torch.Size([54, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([55, 3, 224, 224])\n","torch.Size([55, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([51, 3, 224, 224])\n","torch.Size([51, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([62, 3, 224, 224])\n","torch.Size([62, 512])\n","For dummy input images with shape: (2, 3, 224, 224)\n","Shape of p3 features: torch.Size([16, 128, 112, 112])\n","Shape of p4 features: torch.Size([16, 128, 56, 56])\n","Shape of p5 features: torch.Size([16, 128, 28, 28])\n","Shape of p6 features: torch.Size([16, 128, 14, 14])\n","Shape of p7 features: torch.Size([16, 128, 7, 7])\n","dict_keys(['p3', 'p4', 'p5', 'p6', 'p7'])\n","torch.Size([75, 3, 224, 224])\n","torch.Size([75, 512])\n"]}],"source":["from torch import nn\n","\n","from a4_helper import train_detector\n","from common import DetectorBackboneWithFPN\n","from FPN_ROI import RPN\n","import clip\n","\n","reset_seed(0)\n","from FPN_ROI import FasterRCNN\n","\n","model, preprocess = clip.load(\"ViT-B/32\")\n","# Slightly larger detector than in above cell.\n","FPN_CHANNELS = 128\n","backbone = DetectorBackboneWithFPN(out_channels=FPN_CHANNELS)\n","rpn = RPN(\n","    fpn_channels=FPN_CHANNELS,\n","    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n","    batch_size_per_image=16,\n","    pre_nms_topk=500,\n","    post_nms_topk=200  # Other args from previous cell are default args in RPN.\n",")\n","# fmt: off\n","faster_rcnn = FasterRCNN(\n","    backbone, rpn, num_classes=NUM_CLASSES, roi_size=(7, 7),\n","    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n","    batch_size_per_image=32,\n","    clip_model = model\n",")\n","# fmt: on\n","\n","train_detector(\n","    faster_rcnn,\n","    train_loader,\n","    learning_rate=0.01,\n","    max_iters=9000,\n","    log_period=50,\n","    device=DEVICE,\n",")\n","\n","# After you've trained your model, save the weights for submission.\n","weights_path = os.path.join('./A4', \"rcnn_detector.pt\")\n","torch.save(faster_rcnn.state_dict(), weights_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QIKs23vzTvN-"},"outputs":[],"source":["import clip\n","import torch\n","\n","clip.available_models()\n","model, preprocess = clip.load(\"ViT-B/32\")\n","cropped_images_all = torch.randn(33,3,224*4,224).to('cuda')\n","# print(cropped_images_all)\n","model.encode_image(cropped_images_all).float()"]}],"metadata":{"interpreter":{"hash":"f18262e1a2788be3d4c180962476641642a277e294ea61ec51e87943ad89b761"},"kernelspec":{"display_name":"Python 3.7.11 ('pytorch')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"orig_nbformat":4,"colab":{"name":"FPN_ROI align.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}